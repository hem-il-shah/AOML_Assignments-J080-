{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7876,"sourceType":"datasetVersion","datasetId":5227}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-08T16:24:53.992073Z","iopub.execute_input":"2025-03-08T16:24:53.992399Z","iopub.status.idle":"2025-03-08T16:24:55.238485Z","shell.execute_reply.started":"2025-03-08T16:24:53.992364Z","shell.execute_reply":"2025-03-08T16:24:55.237395Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/california-housing-prices/housing.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.linear_model import Ridge, Lasso\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\n\n# Load the dataset\ndata = pd.read_csv('/kaggle/input/california-housing-prices/housing.csv')\n\n# Prepare the data\n# Handle missing values\nimputer = SimpleImputer(strategy='median')\nnumerical_cols = data.select_dtypes(include=np.number).columns\ndata[numerical_cols] = imputer.fit_transform(data[numerical_cols])\n\n# Create income categories for stratification\ndata['income_cat'] = pd.cut(data['median_income'],\n                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n                               labels=[1, 2, 3, 4, 5])\n\n# Stratified split using Sturge's Rule (for binning target values)\nn_bins = int(np.ceil(np.log2(len(data['median_house_value'])) + 1))\ndata['house_value_cat'] = pd.cut(data['median_house_value'], bins=n_bins, labels=False)\n\nsplitter = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in splitter.split(data, data['house_value_cat']):\n    train_set = data.loc[train_index]\n    test_set = data.loc[test_index]\n\n# Drop unnecessary categorical and stratification columns\nfor set_ in (train_set, test_set):\n    set_.drop(['income_cat', 'house_value_cat','ocean_proximity'], axis=1, inplace=True)\n\n# Separate features and target\ntrain_features = train_set.drop('median_house_value', axis=1)\ntrain_target = train_set['median_house_value'].copy()\ntest_features = test_set.drop('median_house_value', axis=1)\ntest_target = test_set['median_house_value'].copy()\n\n# Scale the features\nscaler = StandardScaler()\ntrain_features_scaled = scaler.fit_transform(train_features)\ntest_features_scaled = scaler.transform(test_features)\n\n# Ridge Regression\nridge_model = Ridge(alpha=1.0, random_state=42)  # Adjust alpha as needed\nridge_model.fit(train_features_scaled, train_target)\nridge_predictions = ridge_model.predict(test_features_scaled)\n\n# Lasso Regression\nlasso_model = Lasso(alpha=1.0, random_state=42)  # Adjust alpha as needed\nlasso_model.fit(train_features_scaled, train_target)\nlasso_predictions = lasso_model.predict(test_features_scaled)\n\n# Evaluate the models\nridge_mae = mean_absolute_error(test_target, ridge_predictions)\nridge_mse = mean_squared_error(test_target, ridge_predictions)\nridge_rmse = np.sqrt(ridge_mse)\n\nlasso_mae = mean_absolute_error(test_target, lasso_predictions)\nlasso_mse = mean_squared_error(test_target, lasso_predictions)\nlasso_rmse = np.sqrt(lasso_mse)\n\n# Print the results\nprint(\"Ridge Regression:\")\nprint(\"MAE:\", ridge_mae)\nprint(\"MSE:\", ridge_mse)\nprint(\"RMSE:\", ridge_rmse)\n\nprint(\"\\nLasso Regression:\")\nprint(\"MAE:\", lasso_mae)\nprint(\"MSE:\", lasso_mse)\nprint(\"RMSE:\", lasso_rmse)\n\n# Findings\n\nprint(\"\\nFindings:\")\n\nif ridge_rmse < lasso_rmse:\n    print(\"Ridge Regression performs slightly better than Lasso Regression in terms of RMSE.\")\nelse:\n    print(\"Lasso Regression performs slightly better than Ridge Regression in terms of RMSE.\")\n\nif ridge_mae < lasso_mae:\n    print(\"Ridge Regression performs slightly better than Lasso Regression in terms of MAE.\")\nelse:\n    print(\"Lasso Regression performs slightly better than Ridge Regression in terms of MAE.\")\n\nif ridge_mse < lasso_mse:\n    print(\"Ridge Regression performs slightly better than Lasso Regression in terms of MSE.\")\nelse:\n    print(\"Lasso Regression performs slightly better than Ridge Regression in terms of MSE.\")\n\nprint(\"The differences in MAE, MSE, and RMSE between Ridge and Lasso are usually small. The choice of model often depends on whether feature selection is desired. Lasso tends to shrink less important feature coefficients to zero, effectively performing feature selection, while Ridge reduces the magnitude of coefficients without eliminating them.\")\n\nprint(\"Alpha parameter tuning is important in both models. Tuning the alpha parameter will greatly change results. For example, a larger alpha in Lasso will result in more features being eliminated. A larger alpha in Ridge will result in smaller coefficient values.\")\n\nprint(\"Stratified sampling based on median house value bins ensures that both the training and test sets have a similar distribution of house values, which is important for accurate model evaluation.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T16:25:26.863349Z","iopub.execute_input":"2025-03-08T16:25:26.863850Z","iopub.status.idle":"2025-03-08T16:25:28.086680Z","shell.execute_reply.started":"2025-03-08T16:25:26.863812Z","shell.execute_reply":"2025-03-08T16:25:28.085546Z"}},"outputs":[{"name":"stdout","text":"Ridge Regression:\nMAE: 50765.08076294403\nMSE: 4747063660.8474455\nRMSE: 68898.93802409037\n\nLasso Regression:\nMAE: 50764.72973655391\nMSE: 4746941378.151994\nRMSE: 68898.05061213266\n\nFindings:\nLasso Regression performs slightly better than Ridge Regression in terms of RMSE.\nLasso Regression performs slightly better than Ridge Regression in terms of MAE.\nLasso Regression performs slightly better than Ridge Regression in terms of MSE.\nThe differences in MAE, MSE, and RMSE between Ridge and Lasso are usually small. The choice of model often depends on whether feature selection is desired. Lasso tends to shrink less important feature coefficients to zero, effectively performing feature selection, while Ridge reduces the magnitude of coefficients without eliminating them.\nAlpha parameter tuning is important in both models. Tuning the alpha parameter will greatly change results. For example, a larger alpha in Lasso will result in more features being eliminated. A larger alpha in Ridge will result in smaller coefficient values.\nStratified sampling based on median house value bins ensures that both the training and test sets have a similar distribution of house values, which is important for accurate model evaluation.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":482,"sourceType":"datasetVersion","datasetId":228}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-08T15:52:05.734826Z","iopub.execute_input":"2025-03-08T15:52:05.735072Z","iopub.status.idle":"2025-03-08T15:52:07.062864Z","shell.execute_reply.started":"2025-03-08T15:52:05.735050Z","shell.execute_reply":"2025-03-08T15:52:07.061423Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/pima-indians-diabetes-database/diabetes.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nimport joblib\n\n# Load the dataset\ndata = pd.read_csv('/kaggle/input/pima-indians-diabetes-database/diabetes.csv')\n\n# Create BMI_category\ndef categorize_bmi(bmi):\n    if bmi < 18.5:\n        return 'Underweight'\n    elif 18.5 <= bmi < 25:\n        return 'Normal'\n    elif 25 <= bmi < 30:\n        return 'Overweight'\n    else:\n        return 'Obese'\n\ndata['BMI_category'] = data['BMI'].apply(categorize_bmi)\n\n# Save the updated DataFrame to a CSV file\ndata.to_csv('diabetes_with_bmi_category.csv', index=False)\n\n# Split data\ntrain_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n\n# Separate features and target\nX_train = train_data.drop('Outcome', axis=1)\ny_train = train_data['Outcome']\nX_val = val_data.drop('Outcome', axis=1)\ny_val = val_data['Outcome']\n\n# Preprocessing pipeline\nnumeric_features = X_train.select_dtypes(include=['number']).columns.tolist()\ncategorical_features = ['BMI_category']\n\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())])\n\ncategorical_transformer = OneHotEncoder(handle_unknown='ignore')\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ])\n\n# Define models and their parameter grids for GridSearchCV\nmodels = {\n    'KNN': (KNeighborsClassifier(), {'model__n_neighbors': list(range(1, 20)), 'model__weights': ['uniform', 'distance']}),\n    'Decision Tree': (DecisionTreeClassifier(random_state=42), {'model__max_depth': list(range(1, 20)), 'model__min_samples_split': [2, 5, 10]}),\n    'Random Forest': (RandomForestClassifier(random_state=42), {'model__n_estimators': [50, 100, 200, 500], 'model__max_depth': list(range(1, 20)), 'model__min_samples_split': [2, 5, 10]}),\n    'Logistic Regression': (LogisticRegression(random_state=42, max_iter=10000), {'model__C': [0.001, 0.01, 0.1, 1, 10, 100], 'model__penalty': ['l1', 'l2']}),\n    'SVM': (SVC(random_state=42), {'model__C': [0.1, 1, 10, 100], 'model__kernel': ['linear', 'rbf', 'poly'], 'model__degree': [1, 2, 3]})\n}\n\nbest_model = None\nbest_accuracy = 0\n\n# Perform GridSearchCV for each model\nfor model_name, (model, param_grid) in models.items():\n    pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])\n    grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n    grid_search.fit(X_train, y_train)\n    y_pred = grid_search.predict(X_val)\n    accuracy = accuracy_score(y_val, y_pred)\n    print(f\"{model_name} Best Parameters: {grid_search.best_params_}, Accuracy: {accuracy}\")\n    if accuracy > best_accuracy:\n        best_accuracy = accuracy\n        best_model = grid_search.best_estimator_\n\n# Ensemble Model\nestimators = [\n    ('KNN', KNeighborsClassifier(n_neighbors=5)),\n    ('Decision Tree', DecisionTreeClassifier(max_depth=7, random_state=42)),\n    ('Random Forest', RandomForestClassifier(n_estimators=200, max_depth=9, random_state=42)),\n    ('Logistic Regression', LogisticRegression(C=1, random_state=42, max_iter=10000)),\n    ('SVM', SVC(C=10, kernel='linear', probability=True, random_state=42)) # Set probability=True\n]\n\nensemble_model = VotingClassifier(estimators=estimators, voting='soft')\nensemble_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('ensemble', ensemble_model)])\nensemble_pipeline.fit(X_train, y_train)\nensemble_pred = ensemble_pipeline.predict(X_val)\nensemble_accuracy = accuracy_score(y_val, ensemble_pred)\nprint(f\"Ensemble Model Accuracy: {ensemble_accuracy}\")\n\nif ensemble_accuracy > best_accuracy:\n    best_accuracy = ensemble_accuracy\n    best_model = ensemble_pipeline\n\nprint(f\"Best Model: {best_model.steps[-1][1] if not isinstance(best_model, Pipeline) else 'Ensemble'}, Best Accuracy: {best_accuracy}\")\n\n# Save preprocessor and model\njoblib.dump(best_model.named_steps['preprocessor'], 'preprocessor.joblib')\nif isinstance(best_model, Pipeline):\n    if 'ensemble' in best_model.named_steps:\n        joblib.dump(best_model.named_steps['ensemble'], 'best_model.joblib')\n    else:\n        joblib.dump(best_model.named_steps['model'], 'best_model.joblib')\nelse:\n    joblib.dump(best_model, 'best_model.joblib')\n\n# Inference Pipeline\ndef predict_diabetes(sample_data):\n    preprocessor = joblib.load('preprocessor.joblib')\n    if isinstance(best_model, Pipeline):\n        if 'ensemble' in best_model.named_steps:\n            model = joblib.load('best_model.joblib')\n        else:\n            model = joblib.load('best_model.joblib')\n    else:\n        model = joblib.load('best_model.joblib')\n\n    sample_df = pd.DataFrame([sample_data])\n    sample_df['BMI_category'] = sample_df['BMI'].apply(categorize_bmi)\n\n    processed_sample = preprocessor.transform(sample_df)\n    if isinstance(best_model, Pipeline):\n        prediction = model.predict(processed_sample)[0]\n    else:\n        prediction = model.predict(processed_sample)[0]\n    return prediction\n\n# Test with 5 samples from validation set\nfor i in range(5):\n    sample = X_val.iloc[i].to_dict()\n    prediction = predict_diabetes(sample)\n    print(f\"Sample {i+1} Prediction: {prediction}, Actual: {y_val.iloc[i]}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-08T15:55:19.597779Z","iopub.execute_input":"2025-03-08T15:55:19.598163Z","iopub.status.idle":"2025-03-08T16:03:36.328136Z","shell.execute_reply.started":"2025-03-08T15:55:19.598137Z","shell.execute_reply":"2025-03-08T16:03:36.326884Z"}},"outputs":[{"name":"stdout","text":"KNN Best Parameters: {'model__n_neighbors': 7, 'model__weights': 'uniform'}, Accuracy: 0.7207792207792207\nDecision Tree Best Parameters: {'model__max_depth': 3, 'model__min_samples_split': 2}, Accuracy: 0.7597402597402597\nRandom Forest Best Parameters: {'model__max_depth': 6, 'model__min_samples_split': 2, 'model__n_estimators': 200}, Accuracy: 0.7597402597402597\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n30 fits failed out of a total of 60.\nThe score on these train-test partitions for these parameters will be set to nan.\nIf these failures are not expected, you can try to debug them by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n30 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 405, in fit\n    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 1162, in fit\n    solver = _check_solver(self.solver, self.penalty, self.dual)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\", line 54, in _check_solver\n    raise ValueError(\nValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n\n  warnings.warn(some_fits_failed_message, FitFailedWarning)\n/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.65309876        nan 0.75081967        nan 0.76389444\n        nan 0.77037185        nan 0.76548047        nan 0.76385446]\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Logistic Regression Best Parameters: {'model__C': 1, 'model__penalty': 'l2'}, Accuracy: 0.7467532467532467\nSVM Best Parameters: {'model__C': 10, 'model__degree': 1, 'model__kernel': 'linear'}, Accuracy: 0.7662337662337663\nEnsemble Model Accuracy: 0.7662337662337663\nBest Model: Ensemble, Best Accuracy: 0.7662337662337663\nSample 1 Prediction: 0, Actual: 0\nSample 2 Prediction: 0, Actual: 0\nSample 3 Prediction: 0, Actual: 0\nSample 4 Prediction: 0, Actual: 0\nSample 5 Prediction: 0, Actual: 0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}